Graph[rank=1](fw, gid=0) [
    Node[rank=1](op=placeholder, name=arg0_1, target='arg0_1', args=(), kwargs={}, tensor_meta=int64[32768], self.meta={'tensor_meta': TensorMetadata(shape=(32768,), dtype=torch.int64, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={})})
    Node[rank=1](op=placeholder, name=arg1_1, target='arg1_1', args=(), kwargs={}, tensor_meta=bfloat16[75968, 1536], self.meta={'tensor_meta': TensorMetadata(shape=(75968, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1536, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})})
    Node[rank=1](op=placeholder, name=arg2_1, target='arg2_1', args=(), kwargs={}, tensor_meta=bfloat16[1536], self.meta={'tensor_meta': TensorMetadata(shape=(1536,), dtype=torch.bfloat16, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={})})
    Node[rank=1](op=placeholder, name=arg3_1, target='arg3_1', args=(), kwargs={}, tensor_meta=bfloat16[1024], self.meta={'tensor_meta': TensorMetadata(shape=(1024,), dtype=torch.bfloat16, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={})})
    Node[rank=1](op=placeholder, name=arg4_1, target='arg4_1', args=(), kwargs={}, tensor_meta=bfloat16[1024, 1536], self.meta={'tensor_meta': TensorMetadata(shape=(1024, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1536, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})})
    Node[rank=1](op=placeholder, name=arg5_1, target='arg5_1', args=(), kwargs={}, tensor_meta=bfloat16[32768, 128], self.meta={'tensor_meta': TensorMetadata(shape=(32768, 128), dtype=torch.bfloat16, requires_grad=False, stride=(128, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})})
    Node[rank=1](op=placeholder, name=arg6_1, target='arg6_1', args=(), kwargs={}, tensor_meta=int64[32768], self.meta={'tensor_meta': TensorMetadata(shape=(32768,), dtype=torch.int64, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={})})
    Node[rank=1](op=placeholder, name=arg7_1, target='arg7_1', args=(), kwargs={}, tensor_meta=bfloat16[1536, 768], self.meta={'tensor_meta': TensorMetadata(shape=(1536, 768), dtype=torch.bfloat16, requires_grad=False, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})})
    Node[rank=1](op=placeholder, name=arg8_1, target='arg8_1', args=(), kwargs={}, tensor_meta=bfloat16[1536], self.meta={'tensor_meta': TensorMetadata(shape=(1536,), dtype=torch.bfloat16, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={})})
    Node[rank=1](op=placeholder, name=arg9_1, target='arg9_1', args=(), kwargs={}, tensor_meta=bfloat16[8960, 1536], self.meta={'tensor_meta': TensorMetadata(shape=(8960, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1536, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})})
    Node[rank=1](op=placeholder, name=arg10_1, target='arg10_1', args=(), kwargs={}, tensor_meta=bfloat16[1536, 4480], self.meta={'tensor_meta': TensorMetadata(shape=(1536, 4480), dtype=torch.bfloat16, requires_grad=False, stride=(4480, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})})
    Node[rank=1](op=placeholder, name=arg11_1, target='arg11_1', args=(), kwargs={}, tensor_meta=bfloat16[1536], self.meta={'tensor_meta': TensorMetadata(shape=(1536,), dtype=torch.bfloat16, requires_grad=True, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={})})
    Node[rank=1](op=call_function, name=ge, target='aten.ge.Scalar', args=('n__r1__arg0_1', 75968), kwargs={}, tensor_meta=bool[32768], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['ge'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 406, in forward\n    masked_input, input_mask = get_masked_input_and_mask(\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 146, in get_masked_input_and_mask\n    org_vocab_mask = (input_ >= org_vocab_start_index) & (\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768,), dtype=torch.bool, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('ge', '<built-in function ge>')], 'from_node': [('ge', '<built-in function ge>')]})
    Node[rank=1](op=call_function, name=lt, target='aten.lt.Scalar', args=('n__r1__arg0_1', 151936), kwargs={}, tensor_meta=bool[32768], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['lt'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 406, in forward\n    masked_input, input_mask = get_masked_input_and_mask(\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 147, in get_masked_input_and_mask\n    input_ < org_vocab_end_index)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768,), dtype=torch.bool, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('lt', '<built-in function lt>')], 'from_node': [('lt', '<built-in function lt>')]})
    Node[rank=1](op=call_function, name=bitwise_and, target='aten.bitwise_and.Tensor', args=('n__r1__ge', 'n__r1__lt'), kwargs={}, tensor_meta=bool[32768], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['and_'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 406, in forward\n    masked_input, input_mask = get_masked_input_and_mask(\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 146, in get_masked_input_and_mask\n    org_vocab_mask = (input_ >= org_vocab_start_index) & (\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768,), dtype=torch.bool, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('and_', '<built-in function and_>')], 'from_node': [('org_vocab_mask', '<built-in function and_>')]})
    Node[rank=1](op=call_function, name=ge_1, target='aten.ge.Scalar', args=('n__r1__arg0_1', 151936), kwargs={}, tensor_meta=bool[32768], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['ge_1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 406, in forward\n    masked_input, input_mask = get_masked_input_and_mask(\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 148, in get_masked_input_and_mask\n    added_vocab_mask = (input_ >= added_vocab_start_index) & (\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768,), dtype=torch.bool, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('ge_1', '<built-in function ge>')], 'from_node': [('ge_1', '<built-in function ge>')]})
    Node[rank=1](op=call_function, name=lt_1, target='aten.lt.Scalar', args=('n__r1__arg0_1', 151936), kwargs={}, tensor_meta=bool[32768], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['lt_1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 406, in forward\n    masked_input, input_mask = get_masked_input_and_mask(\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 149, in get_masked_input_and_mask\n    input_ < added_vocab_end_index)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768,), dtype=torch.bool, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('lt_1', '<built-in function lt>')], 'from_node': [('lt_1', '<built-in function lt>')]})
    Node[rank=1](op=call_function, name=bitwise_and_1, target='aten.bitwise_and.Tensor', args=('n__r1__ge_1', 'n__r1__lt_1'), kwargs={}, tensor_meta=bool[32768], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['and__1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 406, in forward\n    masked_input, input_mask = get_masked_input_and_mask(\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 148, in get_masked_input_and_mask\n    added_vocab_mask = (input_ >= added_vocab_start_index) & (\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768,), dtype=torch.bool, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('and__1', '<built-in function and_>')], 'from_node': [('added_vocab_mask', '<built-in function and_>')]})
    Node[rank=1](op=call_function, name=mul, target='aten.mul.Tensor', args=('n__r1__bitwise_and', 75968), kwargs={}, tensor_meta=int64[32768], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['mul'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 406, in forward\n    masked_input, input_mask = get_masked_input_and_mask(\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 152, in get_masked_input_and_mask\n    valid_offset = (org_vocab_start_index *\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768,), dtype=torch.int64, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('mul', '<built-in function mul>')], 'from_node': [('mul', '<built-in function mul>')]})
    Node[rank=1](op=call_function, name=mul_1, target='aten.mul.Tensor', args=('n__r1__bitwise_and_1', 75968), kwargs={}, tensor_meta=int64[32768], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['mul_1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 406, in forward\n    masked_input, input_mask = get_masked_input_and_mask(\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 153, in get_masked_input_and_mask\n    org_vocab_mask) + (added_offset * added_vocab_mask)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768,), dtype=torch.int64, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('mul_1', '<built-in function mul>')], 'from_node': [('mul_1', '<built-in function mul>')]})
    Node[rank=1](op=call_function, name=add, target='aten.add.Tensor', args=('n__r1__mul', 'n__r1__mul_1'), kwargs={}, tensor_meta=int64[32768], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['add'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 406, in forward\n    masked_input, input_mask = get_masked_input_and_mask(\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 152, in get_masked_input_and_mask\n    valid_offset = (org_vocab_start_index *\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768,), dtype=torch.int64, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('add', '<built-in function add>')], 'from_node': [('valid_offset', '<built-in function add>')]})
    Node[rank=1](op=call_function, name=bitwise_or, target='aten.bitwise_or.Tensor', args=('n__r1__bitwise_and', 'n__r1__bitwise_and_1'), kwargs={}, tensor_meta=bool[32768], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['or_'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 406, in forward\n    masked_input, input_mask = get_masked_input_and_mask(\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 154, in get_masked_input_and_mask\n    vocab_mask = org_vocab_mask | added_vocab_mask\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768,), dtype=torch.bool, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('or_', '<built-in function or_>')], 'from_node': [('vocab_mask', '<built-in function or_>')]})
    Node[rank=1](op=call_function, name=sub, target='aten.sub.Tensor', args=('n__r1__arg0_1', 'n__r1__add'), kwargs={}, tensor_meta=int64[32768], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['sub'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 406, in forward\n    masked_input, input_mask = get_masked_input_and_mask(\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 155, in get_masked_input_and_mask\n    input_ = vocab_mask * (input_ - valid_offset)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768,), dtype=torch.int64, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('sub', '<built-in function sub>')], 'from_node': [('sub', '<built-in function sub>')]})
    Node[rank=1](op=call_function, name=mul_2, target='aten.mul.Tensor', args=('n__r1__bitwise_or', 'n__r1__sub'), kwargs={}, tensor_meta=int64[32768], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['mul_2'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 406, in forward\n    masked_input, input_mask = get_masked_input_and_mask(\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 155, in get_masked_input_and_mask\n    input_ = vocab_mask * (input_ - valid_offset)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768,), dtype=torch.int64, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('mul_2', '<built-in function mul>')], 'from_node': [('input_', '<built-in function mul>')]})
    Node[rank=1](op=call_function, name=bitwise_not, target='aten.bitwise_not.default', args=('n__r1__bitwise_or',), kwargs={}, tensor_meta=bool[32768], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['invert'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 406, in forward\n    masked_input, input_mask = get_masked_input_and_mask(\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 156, in get_masked_input_and_mask\n    return input_, ~vocab_mask\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768,), dtype=torch.bool, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('invert', '<built-in function invert>')], 'from_node': [('input_mask', '<built-in function invert>')]})
    Node[rank=1](op=call_function, name=embedding, target='aten.embedding.default', args=('n__r1__arg1_1', 'n__r1__mul_2'), kwargs={}, tensor_meta=bfloat16[32768, 1536], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['embedding'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 415, in forward\n    output_parallel = self.quant_method.embedding(self,\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 47, in embedding\n    return F.embedding(input_, layer.weight)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1536, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('embedding', '<function embedding at 0x7f9dc77a3380>')], 'from_node': [('output_parallel', '<function embedding at 0x7f9dc77a3380>')]})
    Node[rank=1](op=call_function, name=unsqueeze, target='aten.unsqueeze.default', args=('n__r1__bitwise_not', -1), kwargs={}, tensor_meta=bool[32768, 1], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['unsqueeze'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 419, in forward\n    output_parallel.masked_fill_(input_mask.unsqueeze(-1), 0)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1), dtype=torch.bool, requires_grad=False, stride=(1, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('unsqueeze', "'unsqueeze'")], 'from_node': [('unsqueeze', "'unsqueeze'")]})
    Node[rank=1](op=call_function, name=masked_fill, target='aten.masked_fill.Scalar', args=('n__r1__embedding', 'n__r1__unsqueeze', 0), kwargs={}, tensor_meta=bfloat16[32768, 1536], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['masked_fill_'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 419, in forward\n    output_parallel.masked_fill_(input_mask.unsqueeze(-1), 0)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1536, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('masked_fill_', "'masked_fill_'")], 'from_node': [('masked_fill_', "'masked_fill_'")]})
    Node[rank=1](op=call_function, name=all_reduce, target='vllm.all_reduce.default', args=('n__r1__masked_fill', 'tp:0'), kwargs={}, tensor_meta=bfloat16[32768, 1536], nn_stack=["L['self'].model", "L['self'].model.embed_tokens"], fn_stack=['all_reduce'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 340, in forward\n    hidden_states = self.get_input_embeddings(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 325, in get_input_embeddings\n    return self.embed_tokens(input_ids)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/vocab_parallel_embedding.py", line 421, in forward\n    output = tensor_model_parallel_all_reduce(output_parallel)\n  File "/opt/tiger/vllm/vllm/distributed/communication_op.py", line 13, in tensor_model_parallel_all_reduce\n    return get_tp_group().all_reduce(input_)\n  File "/opt/tiger/vllm/vllm/distributed/parallel_state.py", line 307, in all_reduce\n    return torch.ops.vllm.all_reduce(input_,\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1536, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316436827072': ("L['self'].model.embed_tokens", "<class 'vllm.model_executor.layers.vocab_parallel_embedding.VocabParallelEmbedding'>")}, 'source_fn_stack': [('all_reduce', "<OpOverloadPacket(op='vllm.all_reduce')>")], 'from_node': [('output', "<OpOverloadPacket(op='vllm.all_reduce')>")]})
    Node[rank=1](op=call_function, name=empty_like, target='aten.empty_like.default', args=('n__r1__all_reduce',), kwargs={'pin_memory': False}, tensor_meta=bfloat16[32768, 1536], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].input_layernorm"], fn_stack=['empty_like'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 243, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/layernorm.py", line 94, in forward_cuda\n    out = torch.empty_like(x)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1536, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140317440810000': ("L['self']._modules['model']._modules['layers']._modules['0'].input_layernorm", "<class 'vllm.model_executor.layers.layernorm.RMSNorm'>")}, 'source_fn_stack': [('empty_like', '<built-in method empty_like of type object at 0x7f9eb525e280>')], 'from_node': [('out', '<built-in method empty_like of type object at 0x7f9eb525e280>')]})
    Node[rank=1](op=call_function, name=detach, target='aten.detach.default', args=('n__r1__arg2_1',), kwargs={}, tensor_meta=bfloat16[1536], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].input_layernorm"], fn_stack=['_get_data_attr'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 243, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/layernorm.py", line 98, in forward_cuda\n    self.weight.data,\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(1536,), dtype=torch.bfloat16, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140317440810000': ("L['self']._modules['model']._modules['layers']._modules['0'].input_layernorm", "<class 'vllm.model_executor.layers.layernorm.RMSNorm'>")}, 'source_fn_stack': [('_get_data_attr', '<built-in method _get_data_attr of PyCapsule object at 0x7f9dc76ffc60>')], 'from_node': [('_get_data_attr', '<built-in method _get_data_attr of PyCapsule object at 0x7f9dc76ffc60>')]})
    Node[rank=1](op=call_function, name=auto_functionalized, target='auto_functionalized', args=('_C.rms_norm.default',), kwargs={'result': 'n__r1__empty_like', 'input': 'n__r1__all_reduce', 'weight': 'n__r1__detach', 'epsilon': 1e-06}, nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].input_layernorm"], fn_stack=['rms_norm'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 243, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/layernorm.py", line 95, in forward_cuda\n    ops.rms_norm(\n  File "/opt/tiger/vllm/vllm/_custom_ops.py", line 153, in rms_norm\n    torch.ops._C.rms_norm(out, input, weight, epsilon)\n', 'seq_nr': 15, 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140317440810000': ("L['self']._modules['model']._modules['layers']._modules['0'].input_layernorm", "<class 'vllm.model_executor.layers.layernorm.RMSNorm'>")}, 'source_fn_stack': [('rms_norm', "<OpOverloadPacket(op='_C.rms_norm')>")], 'from_node': [('rms_norm', "<OpOverloadPacket(op='_C.rms_norm')>")]})
    Node[rank=1](op=call_function, name=getitem_1, target='<built-in function getitem>', args=('n__r1__auto_functionalized', 1), kwargs={}, tensor_meta=bfloat16[32768, 1536], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].input_layernorm"], fn_stack=['rms_norm'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 243, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/layernorm.py", line 95, in forward_cuda\n    ops.rms_norm(\n  File "/opt/tiger/vllm/vllm/_custom_ops.py", line 153, in rms_norm\n    torch.ops._C.rms_norm(out, input, weight, epsilon)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1536, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140317440810000': ("L['self']._modules['model']._modules['layers']._modules['0'].input_layernorm", "<class 'vllm.model_executor.layers.layernorm.RMSNorm'>")}, 'source_fn_stack': [('rms_norm', "<OpOverloadPacket(op='_C.rms_norm')>")], 'from_node': [('rms_norm', "<OpOverloadPacket(op='_C.rms_norm')>")]})
    Node[rank=1](op=call_function, name=t, target='aten.t.default', args=('n__r1__arg4_1',), kwargs={}, tensor_meta=bfloat16[1536, 1024], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.qkv_proj"], fn_stack=['linear'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 176, in forward\n    qkv, _ = self.qkv_proj(hidden_states)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 388, in forward\n    output_parallel = self.quant_method.apply(self, input_, bias)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 142, in apply\n    return F.linear(x, layer.weight, bias)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(1536, 1024), dtype=torch.bfloat16, requires_grad=False, stride=(1, 1536), memory_format=None, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316438739424': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.qkv_proj", "<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>")}, 'source_fn_stack': [('linear', '<built-in function linear>')], 'from_node': [('output_parallel_1', '<built-in function linear>')]})
    Node[rank=1](op=call_function, name=addmm, target='aten.addmm.default', args=('n__r1__arg3_1', 'n__r1__getitem_1', 'n__r1__t'), kwargs={}, tensor_meta=bfloat16[32768, 1024], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.qkv_proj"], fn_stack=['linear'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 176, in forward\n    qkv, _ = self.qkv_proj(hidden_states)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 388, in forward\n    output_parallel = self.quant_method.apply(self, input_, bias)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 142, in apply\n    return F.linear(x, layer.weight, bias)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1024), dtype=torch.bfloat16, requires_grad=False, stride=(1024, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316438739424': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.qkv_proj", "<class 'vllm.model_executor.layers.linear.QKVParallelLinear'>")}, 'source_fn_stack': [('linear', '<built-in function linear>')], 'from_node': [('output_parallel_1', '<built-in function linear>')]})
    Node[rank=1](op=call_function, name=split_with_sizes, target='aten.split_with_sizes.default', args=('n__r1__addmm', [768, 128, 128], -1), kwargs={}, nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn"], fn_stack=['split'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 177, in forward\n    q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n', 'seq_nr': 15, 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>")}, 'source_fn_stack': [('split', "'split'")], 'from_node': [('split', "'split'")]})
    Node[rank=1](op=call_function, name=getitem_2, target='<built-in function getitem>', args=('n__r1__split_with_sizes', 0), kwargs={}, tensor_meta=bfloat16[32768, 768], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn"], fn_stack=['split'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 177, in forward\n    q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 768), dtype=torch.bfloat16, requires_grad=False, stride=(1024, 1), memory_format=None, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>")}, 'source_fn_stack': [('split', "'split'")], 'from_node': [('split', "'split'")]})
    Node[rank=1](op=call_function, name=getitem_3, target='<built-in function getitem>', args=('n__r1__split_with_sizes', 1), kwargs={}, tensor_meta=bfloat16[32768, 128], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn"], fn_stack=['split'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 177, in forward\n    q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 128), dtype=torch.bfloat16, requires_grad=False, stride=(1024, 1), memory_format=None, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>")}, 'source_fn_stack': [('split', "'split'")], 'from_node': [('split', "'split'")]})
    Node[rank=1](op=call_function, name=auto_functionalized_1, target='auto_functionalized', args=('_C.rotary_embedding.default',), kwargs={'positions': 'n__r1__arg6_1', 'query': 'n__r1__getitem_2', 'key': 'n__r1__getitem_3', 'head_size': 128, 'cos_sin_cache': 'n__r1__arg5_1', 'is_neox': True}, nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.rotary_emb"], fn_stack=['rotary_embedding'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 178, in forward\n    q, k = self.rotary_emb(positions, q, k)\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/rotary_embedding.py", line 173, in forward_cuda\n    ops.rotary_embedding(positions, query, key, self.head_size,\n  File "/opt/tiger/vllm/vllm/_custom_ops.py", line 136, in rotary_embedding\n    torch.ops._C.rotary_embedding(positions, query, key, head_size,\n', 'seq_nr': 15, 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316442288944': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.rotary_emb", "<class 'vllm.model_executor.layers.rotary_embedding.RotaryEmbedding'>")}, 'source_fn_stack': [('rotary_embedding', "<OpOverloadPacket(op='_C.rotary_embedding')>")], 'from_node': [('rotary_embedding', "<OpOverloadPacket(op='_C.rotary_embedding')>")]})
    Node[rank=1](op=call_function, name=getitem_6, target='<built-in function getitem>', args=('n__r1__auto_functionalized_1', 1), kwargs={}, tensor_meta=bfloat16[32768, 768], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.rotary_emb"], fn_stack=['rotary_embedding'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 178, in forward\n    q, k = self.rotary_emb(positions, q, k)\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/rotary_embedding.py", line 173, in forward_cuda\n    ops.rotary_embedding(positions, query, key, self.head_size,\n  File "/opt/tiger/vllm/vllm/_custom_ops.py", line 136, in rotary_embedding\n    torch.ops._C.rotary_embedding(positions, query, key, head_size,\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 768), dtype=torch.bfloat16, requires_grad=False, stride=(1024, 1), memory_format=None, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316442288944': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.rotary_emb", "<class 'vllm.model_executor.layers.rotary_embedding.RotaryEmbedding'>")}, 'source_fn_stack': [('rotary_embedding', "<OpOverloadPacket(op='_C.rotary_embedding')>")], 'from_node': [('rotary_embedding', "<OpOverloadPacket(op='_C.rotary_embedding')>")]})
    Node[rank=1](op=call_function, name=getitem_7, target='<built-in function getitem>', args=('n__r1__auto_functionalized_1', 2), kwargs={}, tensor_meta=bfloat16[32768, 128], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.rotary_emb"], fn_stack=['rotary_embedding'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 178, in forward\n    q, k = self.rotary_emb(positions, q, k)\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/rotary_embedding.py", line 173, in forward_cuda\n    ops.rotary_embedding(positions, query, key, self.head_size,\n  File "/opt/tiger/vllm/vllm/_custom_ops.py", line 136, in rotary_embedding\n    torch.ops._C.rotary_embedding(positions, query, key, head_size,\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 128), dtype=torch.bfloat16, requires_grad=False, stride=(1024, 1), memory_format=None, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316442288944': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.rotary_emb", "<class 'vllm.model_executor.layers.rotary_embedding.RotaryEmbedding'>")}, 'source_fn_stack': [('rotary_embedding', "<OpOverloadPacket(op='_C.rotary_embedding')>")], 'from_node': [('rotary_embedding', "<OpOverloadPacket(op='_C.rotary_embedding')>")]})
    Node[rank=1](op=call_function, name=slice_scatter, target='aten.slice_scatter.default', args=('n__r1__addmm', 'n__r1__getitem_6', 1, 0, 768), kwargs={}, tensor_meta=bfloat16[32768, 1024], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.rotary_emb"], fn_stack=['rotary_embedding'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 178, in forward\n    q, k = self.rotary_emb(positions, q, k)\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/rotary_embedding.py", line 173, in forward_cuda\n    ops.rotary_embedding(positions, query, key, self.head_size,\n  File "/opt/tiger/vllm/vllm/_custom_ops.py", line 136, in rotary_embedding\n    torch.ops._C.rotary_embedding(positions, query, key, head_size,\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1024), dtype=torch.bfloat16, requires_grad=False, stride=(1024, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316442288944': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.rotary_emb", "<class 'vllm.model_executor.layers.rotary_embedding.RotaryEmbedding'>")}, 'source_fn_stack': [('rotary_embedding', "<OpOverloadPacket(op='_C.rotary_embedding')>")], 'from_node': [('rotary_embedding', "<OpOverloadPacket(op='_C.rotary_embedding')>")]})
    Node[rank=1](op=call_function, name=slice_scatter_1, target='aten.slice_scatter.default', args=('n__r1__slice_scatter', 'n__r1__getitem_7', 1, 768, 896), kwargs={}, tensor_meta=bfloat16[32768, 1024], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.rotary_emb"], fn_stack=['rotary_embedding'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 178, in forward\n    q, k = self.rotary_emb(positions, q, k)\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/rotary_embedding.py", line 173, in forward_cuda\n    ops.rotary_embedding(positions, query, key, self.head_size,\n  File "/opt/tiger/vllm/vllm/_custom_ops.py", line 136, in rotary_embedding\n    torch.ops._C.rotary_embedding(positions, query, key, head_size,\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1024), dtype=torch.bfloat16, requires_grad=False, stride=(1024, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316442288944': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.rotary_emb", "<class 'vllm.model_executor.layers.rotary_embedding.RotaryEmbedding'>")}, 'source_fn_stack': [('rotary_embedding', "<OpOverloadPacket(op='_C.rotary_embedding')>")], 'from_node': [('rotary_embedding', "<OpOverloadPacket(op='_C.rotary_embedding')>")]})
    Node[rank=1](op=call_function, name=split_with_sizes_3, target='aten.split_with_sizes.default', args=('n__r1__slice_scatter_1', [768, 128, 128], -1), kwargs={}, nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['empty_like_1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 167, in forward\n    output = torch.empty_like(query)\n', 'seq_nr': 15, 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('empty_like_1', '<built-in method empty_like of type object at 0x7f9eb525e280>')], 'from_node': [('output_1', '<built-in method empty_like of type object at 0x7f9eb525e280>')]})
    Node[rank=1](op=call_function, name=getitem_14, target='<built-in function getitem>', args=('n__r1__split_with_sizes_3', 0), kwargs={}, tensor_meta=bfloat16[32768, 768], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['empty_like_1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 167, in forward\n    output = torch.empty_like(query)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 768), dtype=torch.bfloat16, requires_grad=False, stride=(1024, 1), memory_format=None, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('empty_like_1', '<built-in method empty_like of type object at 0x7f9eb525e280>')], 'from_node': [('output_1', '<built-in method empty_like of type object at 0x7f9eb525e280>')]})
    Node[rank=1](op=call_function, name=empty_like_1, target='aten.empty_like.default', args=('n__r1__getitem_14',), kwargs={'pin_memory': False}, tensor_meta=bfloat16[32768, 768], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['empty_like_1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 167, in forward\n    output = torch.empty_like(query)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 768), dtype=torch.bfloat16, requires_grad=False, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('empty_like_1', '<built-in method empty_like of type object at 0x7f9eb525e280>')], 'from_node': [('output_1', '<built-in method empty_like of type object at 0x7f9eb525e280>')]})
    Node[rank=1](op=call_function, name=view_1, target='aten.view.default', args=('n__r1__empty_like_1', [-1, 6, 128]), kwargs={}, tensor_meta=bfloat16[32768, 6, 128], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['view_1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 173, in forward\n    output = output.view(-1, self.num_heads, self.head_size)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 6, 128), dtype=torch.bfloat16, requires_grad=False, stride=(768, 128, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('view_1', "'view'")], 'from_node': [('output_2', "'view'")]})
    Node[rank=1](op=call_function, name=split_with_sizes_4, target='aten.split_with_sizes.default', args=('n__r1__slice_scatter_1', [768, 128, 128], -1), kwargs={}, nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['unified_attention_with_output'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 190, in forward\n    torch.ops.vllm.unified_attention_with_output(\n', 'seq_nr': 15, 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")], 'from_node': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")]})
    Node[rank=1](op=call_function, name=getitem_17, target='<built-in function getitem>', args=('n__r1__split_with_sizes_4', 0), kwargs={}, tensor_meta=bfloat16[32768, 768], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['unified_attention_with_output'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 190, in forward\n    torch.ops.vllm.unified_attention_with_output(\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 768), dtype=torch.bfloat16, requires_grad=False, stride=(1024, 1), memory_format=None, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")], 'from_node': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")]})
    Node[rank=1](op=call_function, name=view_4, target='aten.view.default', args=('n__r1__getitem_17', [-1, 6, 128]), kwargs={}, tensor_meta=bfloat16[32768, 6, 128], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['unified_attention_with_output'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 190, in forward\n    torch.ops.vllm.unified_attention_with_output(\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 6, 128), dtype=torch.bfloat16, requires_grad=False, stride=(1024, 128, 1), memory_format=None, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")], 'from_node': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")]})
    Node[rank=1](op=call_function, name=split_with_sizes_5, target='aten.split_with_sizes.default', args=('n__r1__slice_scatter_1', [768, 128, 128], -1), kwargs={}, nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['unified_attention_with_output'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 190, in forward\n    torch.ops.vllm.unified_attention_with_output(\n', 'seq_nr': 15, 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")], 'from_node': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")]})
    Node[rank=1](op=call_function, name=getitem_21, target='<built-in function getitem>', args=('n__r1__split_with_sizes_5', 1), kwargs={}, tensor_meta=bfloat16[32768, 128], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['unified_attention_with_output'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 190, in forward\n    torch.ops.vllm.unified_attention_with_output(\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 128), dtype=torch.bfloat16, requires_grad=False, stride=(1024, 1), memory_format=None, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")], 'from_node': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")]})
    Node[rank=1](op=call_function, name=view_5, target='aten.view.default', args=('n__r1__getitem_21', [-1, 1, 128]), kwargs={}, tensor_meta=bfloat16[32768, 1, 128], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['unified_attention_with_output'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 190, in forward\n    torch.ops.vllm.unified_attention_with_output(\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1, 128), dtype=torch.bfloat16, requires_grad=False, stride=(1024, 128, 1), memory_format=None, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")], 'from_node': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")]})
    Node[rank=1](op=call_function, name=split_with_sizes_6, target='aten.split_with_sizes.default', args=('n__r1__slice_scatter_1', [768, 128, 128], -1), kwargs={}, nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['unified_attention_with_output'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 190, in forward\n    torch.ops.vllm.unified_attention_with_output(\n', 'seq_nr': 15, 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")], 'from_node': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")]})
    Node[rank=1](op=call_function, name=getitem_25, target='<built-in function getitem>', args=('n__r1__split_with_sizes_6', 2), kwargs={}, tensor_meta=bfloat16[32768, 128], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['unified_attention_with_output'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 190, in forward\n    torch.ops.vllm.unified_attention_with_output(\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 128), dtype=torch.bfloat16, requires_grad=False, stride=(1024, 1), memory_format=None, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")], 'from_node': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")]})
    Node[rank=1](op=call_function, name=view_6, target='aten.view.default', args=('n__r1__getitem_25', [-1, 1, 128]), kwargs={}, tensor_meta=bfloat16[32768, 1, 128], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['unified_attention_with_output'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 190, in forward\n    torch.ops.vllm.unified_attention_with_output(\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1, 128), dtype=torch.bfloat16, requires_grad=False, stride=(1024, 128, 1), memory_format=None, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")], 'from_node': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")]})
    Node[rank=1](op=call_function, name=auto_functionalized_2, target='auto_functionalized', args=('vllm.unified_attention_with_output.default',), kwargs={'query': 'n__r1__view_4', 'key': 'n__r1__view_5', 'value': 'n__r1__view_6', 'output': 'n__r1__view_1', 'layer_name': 'model.layers.0.self_attn.attn'}, nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['unified_attention_with_output'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 190, in forward\n    torch.ops.vllm.unified_attention_with_output(\n', 'seq_nr': 15, 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")], 'from_node': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")]})
    Node[rank=1](op=call_function, name=getitem_27, target='<built-in function getitem>', args=('n__r1__auto_functionalized_2', 1), kwargs={}, tensor_meta=bfloat16[32768, 6, 128], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['unified_attention_with_output'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 190, in forward\n    torch.ops.vllm.unified_attention_with_output(\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 6, 128), dtype=torch.bfloat16, requires_grad=False, stride=(768, 128, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")], 'from_node': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")]})
    Node[rank=1](op=call_function, name=view_7, target='aten.view.default', args=('n__r1__getitem_27', [32768, 768]), kwargs={}, tensor_meta=bfloat16[32768, 768], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn"], fn_stack=['unified_attention_with_output'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 179, in forward\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n  File "/opt/tiger/vllm/vllm/attention/layer.py", line 190, in forward\n    torch.ops.vllm.unified_attention_with_output(\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 768), dtype=torch.bfloat16, requires_grad=False, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316436839888': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.attn", "<class 'vllm.attention.layer.Attention'>")}, 'source_fn_stack': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")], 'from_node': [('unified_attention_with_output', "<OpOverloadPacket(op='vllm.unified_attention_with_output')>")]})
    Node[rank=1](op=call_function, name=t_1, target='aten.t.default', args=('n__r1__arg7_1',), kwargs={}, tensor_meta=bfloat16[768, 1536], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.o_proj"], fn_stack=['linear_1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 180, in forward\n    output, _ = self.o_proj(attn_output)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 1149, in forward\n    output_parallel = self.quant_method.apply(self,\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 142, in apply\n    return F.linear(x, layer.weight, bias)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(768, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1, 768), memory_format=None, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316444803584': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.o_proj", "<class 'vllm.model_executor.layers.linear.RowParallelLinear'>")}, 'source_fn_stack': [('linear_1', '<built-in function linear>')], 'from_node': [('output_parallel_2', '<built-in function linear>')]})
    Node[rank=1](op=call_function, name=view_10, target='aten.view.default', args=('n__r1__view_7', [-1, 6, 128]), kwargs={}, tensor_meta=bfloat16[32768, 6, 128], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.o_proj"], fn_stack=['linear_1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 180, in forward\n    output, _ = self.o_proj(attn_output)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 1149, in forward\n    output_parallel = self.quant_method.apply(self,\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 142, in apply\n    return F.linear(x, layer.weight, bias)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 6, 128), dtype=torch.bfloat16, requires_grad=False, stride=(768, 128, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316444803584': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.o_proj", "<class 'vllm.model_executor.layers.linear.RowParallelLinear'>")}, 'source_fn_stack': [('linear_1', '<built-in function linear>')], 'from_node': [('output_parallel_2', '<built-in function linear>')]})
    Node[rank=1](op=call_function, name=view_11, target='aten.view.default', args=('n__r1__view_10', [-1, 768]), kwargs={}, tensor_meta=bfloat16[32768, 768], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.o_proj"], fn_stack=['linear_1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 180, in forward\n    output, _ = self.o_proj(attn_output)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 1149, in forward\n    output_parallel = self.quant_method.apply(self,\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 142, in apply\n    return F.linear(x, layer.weight, bias)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 768), dtype=torch.bfloat16, requires_grad=False, stride=(768, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316444803584': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.o_proj", "<class 'vllm.model_executor.layers.linear.RowParallelLinear'>")}, 'source_fn_stack': [('linear_1', '<built-in function linear>')], 'from_node': [('output_parallel_2', '<built-in function linear>')]})
    Node[rank=1](op=call_function, name=mm, target='aten.mm.default', args=('n__r1__view_11', 'n__r1__t_1'), kwargs={}, tensor_meta=bfloat16[32768, 1536], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.o_proj"], fn_stack=['linear_1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 180, in forward\n    output, _ = self.o_proj(attn_output)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 1149, in forward\n    output_parallel = self.quant_method.apply(self,\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 142, in apply\n    return F.linear(x, layer.weight, bias)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1536, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316444803584': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.o_proj", "<class 'vllm.model_executor.layers.linear.RowParallelLinear'>")}, 'source_fn_stack': [('linear_1', '<built-in function linear>')], 'from_node': [('output_parallel_2', '<built-in function linear>')]})
    Node[rank=1](op=call_function, name=all_reduce_1, target='vllm.all_reduce.default', args=('n__r1__mm', 'tp:0'), kwargs={}, tensor_meta=bfloat16[32768, 1536], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "L['self']._modules['model']._modules['layers']._modules['0'].self_attn.o_proj"], fn_stack=['all_reduce_1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 247, in forward\n    hidden_states = self.self_attn(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 180, in forward\n    output, _ = self.o_proj(attn_output)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 1153, in forward\n    output = tensor_model_parallel_all_reduce(output_parallel)\n  File "/opt/tiger/vllm/vllm/distributed/communication_op.py", line 13, in tensor_model_parallel_all_reduce\n    return get_tp_group().all_reduce(input_)\n  File "/opt/tiger/vllm/vllm/distributed/parallel_state.py", line 307, in all_reduce\n    return torch.ops.vllm.all_reduce(input_,\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1536, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316438740528': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn", "<class 'vllm.model_executor.models.qwen2.Qwen2Attention'>"), '140316444803584': ("L['self']._modules['model']._modules['layers']._modules['0'].self_attn.o_proj", "<class 'vllm.model_executor.layers.linear.RowParallelLinear'>")}, 'source_fn_stack': [('all_reduce_1', "<OpOverloadPacket(op='vllm.all_reduce')>")], 'from_node': [('output_3', "<OpOverloadPacket(op='vllm.all_reduce')>")]})
    Node[rank=1](op=call_function, name=detach_1, target='aten.detach.default', args=('n__r1__arg8_1',), kwargs={}, tensor_meta=bfloat16[1536], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].post_attention_layernorm"], fn_stack=['_get_data_attr_1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 255, in forward\n    hidden_states, residual = self.post_attention_layernorm(\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/layernorm.py", line 90, in forward_cuda\n    self.weight.data,\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(1536,), dtype=torch.bfloat16, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316440571648': ("L['self']._modules['model']._modules['layers']._modules['0'].post_attention_layernorm", "<class 'vllm.model_executor.layers.layernorm.RMSNorm'>")}, 'source_fn_stack': [('_get_data_attr_1', '<built-in method _get_data_attr of PyCapsule object at 0x7f9dc76ffc60>')], 'from_node': [('_get_data_attr_1', '<built-in method _get_data_attr of PyCapsule object at 0x7f9dc76ffc60>')]})
    Node[rank=1](op=call_function, name=auto_functionalized_3, target='auto_functionalized', args=('_C.fused_add_rms_norm.default',), kwargs={'input': 'n__r1__all_reduce_1', 'residual': 'n__r1__all_reduce', 'weight': 'n__r1__detach_1', 'epsilon': 1e-06}, nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].post_attention_layernorm"], fn_stack=['fused_add_rms_norm'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 255, in forward\n    hidden_states, residual = self.post_attention_layernorm(\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/layernorm.py", line 87, in forward_cuda\n    ops.fused_add_rms_norm(\n  File "/opt/tiger/vllm/vllm/_custom_ops.py", line 158, in fused_add_rms_norm\n    torch.ops._C.fused_add_rms_norm(input, residual, weight, epsilon)\n', 'seq_nr': 15, 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316440571648': ("L['self']._modules['model']._modules['layers']._modules['0'].post_attention_layernorm", "<class 'vllm.model_executor.layers.layernorm.RMSNorm'>")}, 'source_fn_stack': [('fused_add_rms_norm', "<OpOverloadPacket(op='_C.fused_add_rms_norm')>")], 'from_node': [('fused_add_rms_norm', "<OpOverloadPacket(op='_C.fused_add_rms_norm')>")]})
    Node[rank=1](op=call_function, name=getitem_29, target='<built-in function getitem>', args=('n__r1__auto_functionalized_3', 1), kwargs={}, tensor_meta=bfloat16[32768, 1536], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].post_attention_layernorm"], fn_stack=['fused_add_rms_norm'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 255, in forward\n    hidden_states, residual = self.post_attention_layernorm(\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/layernorm.py", line 87, in forward_cuda\n    ops.fused_add_rms_norm(\n  File "/opt/tiger/vllm/vllm/_custom_ops.py", line 158, in fused_add_rms_norm\n    torch.ops._C.fused_add_rms_norm(input, residual, weight, epsilon)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1536, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316440571648': ("L['self']._modules['model']._modules['layers']._modules['0'].post_attention_layernorm", "<class 'vllm.model_executor.layers.layernorm.RMSNorm'>")}, 'source_fn_stack': [('fused_add_rms_norm', "<OpOverloadPacket(op='_C.fused_add_rms_norm')>")], 'from_node': [('fused_add_rms_norm', "<OpOverloadPacket(op='_C.fused_add_rms_norm')>")]})
    Node[rank=1](op=call_function, name=getitem_30, target='<built-in function getitem>', args=('n__r1__auto_functionalized_3', 2), kwargs={}, tensor_meta=bfloat16[32768, 1536], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].post_attention_layernorm"], fn_stack=['fused_add_rms_norm'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 255, in forward\n    hidden_states, residual = self.post_attention_layernorm(\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/layernorm.py", line 87, in forward_cuda\n    ops.fused_add_rms_norm(\n  File "/opt/tiger/vllm/vllm/_custom_ops.py", line 158, in fused_add_rms_norm\n    torch.ops._C.fused_add_rms_norm(input, residual, weight, epsilon)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1536, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316440571648': ("L['self']._modules['model']._modules['layers']._modules['0'].post_attention_layernorm", "<class 'vllm.model_executor.layers.layernorm.RMSNorm'>")}, 'source_fn_stack': [('fused_add_rms_norm', "<OpOverloadPacket(op='_C.fused_add_rms_norm')>")], 'from_node': [('fused_add_rms_norm', "<OpOverloadPacket(op='_C.fused_add_rms_norm')>")]})
    Node[rank=1](op=call_function, name=t_2, target='aten.t.default', args=('n__r1__arg9_1',), kwargs={}, tensor_meta=bfloat16[1536, 8960], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].mlp", "L['self']._modules['model']._modules['layers']._modules['0'].mlp.gate_up_proj"], fn_stack=['linear_2'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 257, in forward\n    hidden_states = self.mlp(hidden_states)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 95, in forward\n    gate_up, _ = self.gate_up_proj(x)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 388, in forward\n    output_parallel = self.quant_method.apply(self, input_, bias)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 142, in apply\n    return F.linear(x, layer.weight, bias)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(1536, 8960), dtype=torch.bfloat16, requires_grad=False, stride=(1, 1536), memory_format=None, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316436824816': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp", "<class 'vllm.model_executor.models.qwen2.Qwen2MLP'>"), '140316440412336': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp.gate_up_proj", "<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>")}, 'source_fn_stack': [('linear_2', '<built-in function linear>')], 'from_node': [('output_parallel_3', '<built-in function linear>')]})
    Node[rank=1](op=call_function, name=mm_1, target='aten.mm.default', args=('n__r1__getitem_29', 'n__r1__t_2'), kwargs={}, tensor_meta=bfloat16[32768, 8960], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].mlp", "L['self']._modules['model']._modules['layers']._modules['0'].mlp.gate_up_proj"], fn_stack=['linear_2'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 257, in forward\n    hidden_states = self.mlp(hidden_states)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 95, in forward\n    gate_up, _ = self.gate_up_proj(x)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 388, in forward\n    output_parallel = self.quant_method.apply(self, input_, bias)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 142, in apply\n    return F.linear(x, layer.weight, bias)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 8960), dtype=torch.bfloat16, requires_grad=False, stride=(8960, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316436824816': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp", "<class 'vllm.model_executor.models.qwen2.Qwen2MLP'>"), '140316440412336': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp.gate_up_proj", "<class 'vllm.model_executor.layers.linear.MergedColumnParallelLinear'>")}, 'source_fn_stack': [('linear_2', '<built-in function linear>')], 'from_node': [('output_parallel_3', '<built-in function linear>')]})
    Node[rank=1](op=call_function, name=empty, target='aten.empty.memory_format', args=([32768, 4480],), kwargs={'dtype': 'torch.bfloat16', 'device': 'cuda:1', 'pin_memory': False}, tensor_meta=bfloat16[32768, 4480], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].mlp", "L['self']._modules['model']._modules['layers']._modules['0'].mlp.act_fn"], fn_stack=['empty'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 257, in forward\n    hidden_states = self.mlp(hidden_states)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 96, in forward\n    x = self.act_fn(gate_up)\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/activation.py", line 81, in forward_cuda\n    out = torch.empty(output_shape, dtype=x.dtype, device=x.device)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 4480), dtype=torch.bfloat16, requires_grad=False, stride=(4480, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316436824816': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp", "<class 'vllm.model_executor.models.qwen2.Qwen2MLP'>"), '140316438732032': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp.act_fn", "<class 'vllm.model_executor.layers.activation.SiluAndMul'>")}, 'source_fn_stack': [('empty', '<built-in method empty of type object at 0x7f9eb525e280>')], 'from_node': [('out_1', '<built-in method empty of type object at 0x7f9eb525e280>')]})
    Node[rank=1](op=call_function, name=auto_functionalized_4, target='auto_functionalized', args=('_C.silu_and_mul.default',), kwargs={'out': 'n__r1__empty', 'input': 'n__r1__mm_1'}, nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].mlp", "L['self']._modules['model']._modules['layers']._modules['0'].mlp.act_fn"], fn_stack=['silu_and_mul'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 257, in forward\n    hidden_states = self.mlp(hidden_states)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 96, in forward\n    x = self.act_fn(gate_up)\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/activation.py", line 82, in forward_cuda\n    self.op(out, x)\n', 'seq_nr': 15, 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316436824816': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp", "<class 'vllm.model_executor.models.qwen2.Qwen2MLP'>"), '140316438732032': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp.act_fn", "<class 'vllm.model_executor.layers.activation.SiluAndMul'>")}, 'source_fn_stack': [('silu_and_mul', "<OpOverloadPacket(op='_C.silu_and_mul')>")], 'from_node': [('silu_and_mul', "<OpOverloadPacket(op='_C.silu_and_mul')>")]})
    Node[rank=1](op=call_function, name=getitem_32, target='<built-in function getitem>', args=('n__r1__auto_functionalized_4', 1), kwargs={}, tensor_meta=bfloat16[32768, 4480], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].mlp", "L['self']._modules['model']._modules['layers']._modules['0'].mlp.act_fn"], fn_stack=['silu_and_mul'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 257, in forward\n    hidden_states = self.mlp(hidden_states)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 96, in forward\n    x = self.act_fn(gate_up)\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/activation.py", line 82, in forward_cuda\n    self.op(out, x)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 4480), dtype=torch.bfloat16, requires_grad=False, stride=(4480, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316436824816': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp", "<class 'vllm.model_executor.models.qwen2.Qwen2MLP'>"), '140316438732032': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp.act_fn", "<class 'vllm.model_executor.layers.activation.SiluAndMul'>")}, 'source_fn_stack': [('silu_and_mul', "<OpOverloadPacket(op='_C.silu_and_mul')>")], 'from_node': [('silu_and_mul', "<OpOverloadPacket(op='_C.silu_and_mul')>")]})
    Node[rank=1](op=call_function, name=t_3, target='aten.t.default', args=('n__r1__arg10_1',), kwargs={}, tensor_meta=bfloat16[4480, 1536], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].mlp", "L['self']._modules['model']._modules['layers']._modules['0'].mlp.down_proj"], fn_stack=['linear_3'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 257, in forward\n    hidden_states = self.mlp(hidden_states)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 97, in forward\n    x, _ = self.down_proj(x)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 1149, in forward\n    output_parallel = self.quant_method.apply(self,\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 142, in apply\n    return F.linear(x, layer.weight, bias)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(4480, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1, 4480), memory_format=None, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316436824816': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp", "<class 'vllm.model_executor.models.qwen2.Qwen2MLP'>"), '140316438478480': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp.down_proj", "<class 'vllm.model_executor.layers.linear.RowParallelLinear'>")}, 'source_fn_stack': [('linear_3', '<built-in function linear>')], 'from_node': [('output_parallel_4', '<built-in function linear>')]})
    Node[rank=1](op=call_function, name=mm_2, target='aten.mm.default', args=('n__r1__getitem_32', 'n__r1__t_3'), kwargs={}, tensor_meta=bfloat16[32768, 1536], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].mlp", "L['self']._modules['model']._modules['layers']._modules['0'].mlp.down_proj"], fn_stack=['linear_3'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 257, in forward\n    hidden_states = self.mlp(hidden_states)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 97, in forward\n    x, _ = self.down_proj(x)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 1149, in forward\n    output_parallel = self.quant_method.apply(self,\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 142, in apply\n    return F.linear(x, layer.weight, bias)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1536, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316436824816': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp", "<class 'vllm.model_executor.models.qwen2.Qwen2MLP'>"), '140316438478480': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp.down_proj", "<class 'vllm.model_executor.layers.linear.RowParallelLinear'>")}, 'source_fn_stack': [('linear_3', '<built-in function linear>')], 'from_node': [('output_parallel_4', '<built-in function linear>')]})
    Node[rank=1](op=call_function, name=all_reduce_2, target='vllm.all_reduce.default', args=('n__r1__mm_2', 'tp:0'), kwargs={}, tensor_meta=bfloat16[32768, 1536], nn_stack=["L['self'].model", "L['self']._modules['model']._modules['layers']._modules['0']", "L['self']._modules['model']._modules['layers']._modules['0'].mlp", "L['self']._modules['model']._modules['layers']._modules['0'].mlp.down_proj"], fn_stack=['all_reduce_2'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 348, in forward\n    hidden_states, residual = layer(\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 257, in forward\n    hidden_states = self.mlp(hidden_states)\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 97, in forward\n    x, _ = self.down_proj(x)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/linear.py", line 1153, in forward\n    output = tensor_model_parallel_all_reduce(output_parallel)\n  File "/opt/tiger/vllm/vllm/distributed/communication_op.py", line 13, in tensor_model_parallel_all_reduce\n    return get_tp_group().all_reduce(input_)\n  File "/opt/tiger/vllm/vllm/distributed/parallel_state.py", line 307, in all_reduce\n    return torch.ops.vllm.all_reduce(input_,\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1536, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316443838848': ("L['self']._modules['model']._modules['layers']._modules['0']", "<class 'vllm.model_executor.models.qwen2.Qwen2DecoderLayer'>"), '140316436824816': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp", "<class 'vllm.model_executor.models.qwen2.Qwen2MLP'>"), '140316438478480': ("L['self']._modules['model']._modules['layers']._modules['0'].mlp.down_proj", "<class 'vllm.model_executor.layers.linear.RowParallelLinear'>")}, 'source_fn_stack': [('all_reduce_2', "<OpOverloadPacket(op='vllm.all_reduce')>")], 'from_node': [('output_4', "<OpOverloadPacket(op='vllm.all_reduce')>")]})
    Node[rank=1](op=call_function, name=detach_2, target='aten.detach.default', args=('n__r1__arg11_1',), kwargs={}, tensor_meta=bfloat16[1536], nn_stack=["L['self'].model", "L['self'].model.norm"], fn_stack=['_get_data_attr_2'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 360, in forward\n    hidden_states, _ = self.norm(hidden_states, residual)\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/layernorm.py", line 90, in forward_cuda\n    self.weight.data,\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(1536,), dtype=torch.bfloat16, requires_grad=False, stride=(1,), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316440165984': ("L['self'].model.norm", "<class 'vllm.model_executor.layers.layernorm.RMSNorm'>")}, 'source_fn_stack': [('_get_data_attr_2', '<built-in method _get_data_attr of PyCapsule object at 0x7f9dc76ffc60>')], 'from_node': [('_get_data_attr_2', '<built-in method _get_data_attr of PyCapsule object at 0x7f9dc76ffc60>')]})
    Node[rank=1](op=call_function, name=auto_functionalized_5, target='auto_functionalized', args=('_C.fused_add_rms_norm.default',), kwargs={'input': 'n__r1__all_reduce_2', 'residual': 'n__r1__getitem_30', 'weight': 'n__r1__detach_2', 'epsilon': 1e-06}, nn_stack=["L['self'].model", "L['self'].model.norm"], fn_stack=['fused_add_rms_norm_1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 360, in forward\n    hidden_states, _ = self.norm(hidden_states, residual)\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/layernorm.py", line 87, in forward_cuda\n    ops.fused_add_rms_norm(\n  File "/opt/tiger/vllm/vllm/_custom_ops.py", line 158, in fused_add_rms_norm\n    torch.ops._C.fused_add_rms_norm(input, residual, weight, epsilon)\n', 'seq_nr': 15, 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316440165984': ("L['self'].model.norm", "<class 'vllm.model_executor.layers.layernorm.RMSNorm'>")}, 'source_fn_stack': [('fused_add_rms_norm_1', "<OpOverloadPacket(op='_C.fused_add_rms_norm')>")], 'from_node': [('fused_add_rms_norm_1', "<OpOverloadPacket(op='_C.fused_add_rms_norm')>")]})
    Node[rank=1](op=call_function, name=getitem_34, target='<built-in function getitem>', args=('n__r1__auto_functionalized_5', 1), kwargs={}, tensor_meta=bfloat16[32768, 1536], nn_stack=["L['self'].model", "L['self'].model.norm"], fn_stack=['fused_add_rms_norm_1'], self.meta={'stack_trace': '  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 476, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File "/opt/tiger/vllm/vllm/model_executor/models/qwen2.py", line 360, in forward\n    hidden_states, _ = self.norm(hidden_states, residual)\n  File "/opt/tiger/vllm/vllm/model_executor/custom_op.py", line 25, in forward\n    return self._forward_method(*args, **kwargs)\n  File "/opt/tiger/vllm/vllm/model_executor/layers/layernorm.py", line 87, in forward_cuda\n    ops.fused_add_rms_norm(\n  File "/opt/tiger/vllm/vllm/_custom_ops.py", line 158, in fused_add_rms_norm\n    torch.ops._C.fused_add_rms_norm(input, residual, weight, epsilon)\n', 'seq_nr': 15, 'tensor_meta': TensorMetadata(shape=(32768, 1536), dtype=torch.bfloat16, requires_grad=False, stride=(1536, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={}), 'nn_module_stack': {'140316437097280': ("L['self'].model", "<class 'vllm.model_executor.models.qwen2.Qwen2Model'>"), '140316440165984': ("L['self'].model.norm", "<class 'vllm.model_executor.layers.layernorm.RMSNorm'>")}, 'source_fn_stack': [('fused_add_rms_norm_1', "<OpOverloadPacket(op='_C.fused_add_rms_norm')>")], 'from_node': [('fused_add_rms_norm_1', "<OpOverloadPacket(op='_C.fused_add_rms_norm')>")]})
    Node[rank=1](op=output, name=output, target='output', args=(('n__r1__getitem_34', 'n__r1__arg5_1'),), kwargs={}, self.meta={})
]


------------------------


